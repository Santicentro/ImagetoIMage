{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6affd8ff-2259-4e3d-ad5c-9c90cdc9f9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Student\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Student\\.cache\\huggingface\\hub\\models--kandinsky-community--kandinsky-2-2-decoder. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 7 files: 100%|██████████████████████████████████████████████████████████████████| 7/7 [03:18<00:00, 28.37s/it]\n",
      "Loading pipeline components...: 100%|████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.67it/s]\n",
      "C:\\Users\\Student\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Student\\.cache\\huggingface\\hub\\models--kandinsky-community--kandinsky-2-2-prior. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 13 files: 100%|███████████████████████████████████████████████████████████████| 13/13 [01:49<00:00,  8.44s/it]\n",
      "Loading pipeline components...: 100%|████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.22it/s]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Refer to https://github.com/facebookresearch/xformers for more information on how to install xformers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m pipeline\u001b[38;5;241m.\u001b[39menable_model_cpu_offload()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_xformers_memory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\diffusers\\pipelines\\kandinsky2_2\\pipeline_kandinsky2_2_combined.py:408\u001b[0m, in \u001b[0;36mKandinskyV22Img2ImgCombinedPipeline.enable_xformers_memory_efficient_attention\u001b[1;34m(self, attention_op)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21menable_xformers_memory_efficient_attention\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_op: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_xformers_memory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_op\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:1635\u001b[0m, in \u001b[0;36mDiffusionPipeline.enable_xformers_memory_efficient_attention\u001b[1;34m(self, attention_op)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21menable_xformers_memory_efficient_attention\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_op: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;124;03m    Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/). When this\u001b[39;00m\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;124;03m    option is enabled, you should observe lower GPU memory usage and a potential speed up during inference. Speed\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1635\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_use_memory_efficient_attention_xformers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_op\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:1661\u001b[0m, in \u001b[0;36mDiffusionPipeline.set_use_memory_efficient_attention_xformers\u001b[1;34m(self, valid, attention_op)\u001b[0m\n\u001b[0;32m   1658\u001b[0m modules \u001b[38;5;241m=\u001b[39m [m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m modules \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)]\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m-> 1661\u001b[0m     \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:1651\u001b[0m, in \u001b[0;36mDiffusionPipeline.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfn_recursive_set_mem_eff\u001b[39m(module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_use_memory_efficient_attention_xformers\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1651\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_use_memory_efficient_attention_xformers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_op\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m   1654\u001b[0m         fn_recursive_set_mem_eff(child)\n",
      "File \u001b[1;32m~\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\diffusers\\models\\modeling_utils.py:273\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers\u001b[1;34m(self, valid, attention_op)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m--> 273\u001b[0m         \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\diffusers\\models\\modeling_utils.py:269\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    266\u001b[0m     module\u001b[38;5;241m.\u001b[39mset_use_memory_efficient_attention_xformers(valid, attention_op)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 269\u001b[0m     \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\diffusers\\models\\modeling_utils.py:269\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    266\u001b[0m     module\u001b[38;5;241m.\u001b[39mset_use_memory_efficient_attention_xformers(valid, attention_op)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 269\u001b[0m     \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\diffusers\\models\\modeling_utils.py:269\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    266\u001b[0m     module\u001b[38;5;241m.\u001b[39mset_use_memory_efficient_attention_xformers(valid, attention_op)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 269\u001b[0m     \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\diffusers\\models\\modeling_utils.py:266\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfn_recursive_set_mem_eff\u001b[39m(module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_use_memory_efficient_attention_xformers\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 266\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_use_memory_efficient_attention_xformers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_op\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m    269\u001b[0m         fn_recursive_set_mem_eff(child)\n",
      "File \u001b[1;32m~\\.conda\\envs\\DiFFusers\\Lib\\site-packages\\diffusers\\models\\attention_processor.py:387\u001b[0m, in \u001b[0;36mAttention.set_use_memory_efficient_attention_xformers\u001b[1;34m(self, use_memory_efficient_attention_xformers, attention_op)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory efficient attention is currently not supported for custom diffusion for attention processor type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m     )\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_xformers_available():\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         (\n\u001b[0;32m    389\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRefer to https://github.com/facebookresearch/xformers for more information on how to install\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    390\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m xformers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         ),\n\u001b[0;32m    392\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxformers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    393\u001b[0m     )\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.cuda.is_available() should be True but is False. xformers\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m memory efficient attention is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m only available for GPU \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    398\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: Refer to https://github.com/facebookresearch/xformers for more information on how to install xformers"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba7771-91ee-47ac-9b0f-0c8250ba4ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e50d67-be48-4284-bfda-a6dffdf47ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\n",
    "image = pipeline(prompt, image=init_image).images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95155e23-8c95-4456-af44-c94e7898d408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ad59ef-0bc5-4833-8b12-044345c8f494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
